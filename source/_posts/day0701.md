## 로지스틱 회귀
- 선형 회귀에서 출발
- 이진 분류 문제 해결
- 클래스 확률 예측
- 딥러닝에서도 사용됨

##교재 177
- X가 사각형일 확률 30%
- X가 삼각형일 확률 50%
- X가 원일 확률 20%

## 데이터 불러오기
- Species (종속변수 = Y)
- 독립변수 Weight, Length, Diagonal, Height, Width


```python
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data') # 데이터를 불러와라
fish.head() # 일단 5개만 출력을 하는데 이름 순서대로 정리가 되어 있다.
```





  <div id="df-4211bf1c-31ee-43d8-8fe4-5c5fdf589c3e">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bream</td>
      <td>242.0</td>
      <td>25.4</td>
      <td>30.0</td>
      <td>11.5200</td>
      <td>4.0200</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bream</td>
      <td>290.0</td>
      <td>26.3</td>
      <td>31.2</td>
      <td>12.4800</td>
      <td>4.3056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Bream</td>
      <td>340.0</td>
      <td>26.5</td>
      <td>31.1</td>
      <td>12.3778</td>
      <td>4.6961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Bream</td>
      <td>363.0</td>
      <td>29.0</td>
      <td>33.5</td>
      <td>12.7300</td>
      <td>4.4555</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Bream</td>
      <td>430.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>12.4440</td>
      <td>5.1340</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Bream</td>
      <td>450.0</td>
      <td>29.7</td>
      <td>34.7</td>
      <td>13.6024</td>
      <td>4.9274</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Bream</td>
      <td>500.0</td>
      <td>29.7</td>
      <td>34.5</td>
      <td>14.1795</td>
      <td>5.2785</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Bream</td>
      <td>390.0</td>
      <td>30.0</td>
      <td>35.0</td>
      <td>12.6700</td>
      <td>4.6900</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Bream</td>
      <td>450.0</td>
      <td>30.0</td>
      <td>35.1</td>
      <td>14.0049</td>
      <td>4.8438</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Bream</td>
      <td>500.0</td>
      <td>30.7</td>
      <td>36.2</td>
      <td>14.2266</td>
      <td>4.9594</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Bream</td>
      <td>475.0</td>
      <td>31.0</td>
      <td>36.2</td>
      <td>14.2628</td>
      <td>5.1042</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Bream</td>
      <td>500.0</td>
      <td>31.0</td>
      <td>36.2</td>
      <td>14.3714</td>
      <td>4.8146</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Bream</td>
      <td>500.0</td>
      <td>31.5</td>
      <td>36.4</td>
      <td>13.7592</td>
      <td>4.3680</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Bream</td>
      <td>340.0</td>
      <td>32.0</td>
      <td>37.3</td>
      <td>13.9129</td>
      <td>5.0728</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Bream</td>
      <td>600.0</td>
      <td>32.0</td>
      <td>37.2</td>
      <td>14.9544</td>
      <td>5.1708</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Bream</td>
      <td>600.0</td>
      <td>32.0</td>
      <td>37.2</td>
      <td>15.4380</td>
      <td>5.5800</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Bream</td>
      <td>700.0</td>
      <td>33.0</td>
      <td>38.3</td>
      <td>14.8604</td>
      <td>5.2854</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Bream</td>
      <td>700.0</td>
      <td>33.0</td>
      <td>38.5</td>
      <td>14.9380</td>
      <td>5.1975</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Bream</td>
      <td>610.0</td>
      <td>33.5</td>
      <td>38.6</td>
      <td>15.6330</td>
      <td>5.1338</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Bream</td>
      <td>650.0</td>
      <td>33.5</td>
      <td>38.7</td>
      <td>14.4738</td>
      <td>5.7276</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-4211bf1c-31ee-43d8-8fe4-5c5fdf589c3e')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-4211bf1c-31ee-43d8-8fe4-5c5fdf589c3e button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-4211bf1c-31ee-43d8-8fe4-5c5fdf589c3e');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python

```

## 데이터 탐색


```python
# 종속변수
print(pd.unique(fish['Species'])) # 물고기의 종류는?
print(fish['Species'].value_counts()) # 각 물고기마다 몇 마리씩 있는가?
```

    ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']
    Perch        56
    Bream        35
    Roach        20
    Pike         17
    Smelt        14
    Parkki       11
    Whitefish     6
    Name: Species, dtype: int64
    


```python
#pandas 데이터 프레임에서 numpy 배열로 변환
fish_input = fish[['Weight', 'Length','Diagonal','Height','Width']].to_numpy()
fish_input.shape
```




    (159, 5)




```python
print(fish_input[:5])
```

    [[242.      25.4     30.      11.52     4.02  ]
     [290.      26.3     31.2     12.48     4.3056]
     [340.      26.5     31.1     12.3778   4.6961]
     [363.      29.      33.5     12.73     4.4555]
     [430.      29.      34.      12.444    5.134 ]]
    


```python
fish_target=fish['Species'].to_numpy() #'Species'만 배열로 만들어라
print(fish_target.shape) #Species 만 배열로 만들었을 때 몇 바이 몇이 되는가?
print(fish_target[:5])# 상위 5개를 추출해봐라
```

    (159,)
    ['Bream' 'Bream' 'Bream' 'Bream' 'Bream']
    

## 데이터 분리
- 훈련 데이터 테스트 데이터 분리



```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state =42
) #fish_input, fish_target 을 넣어서 랜덤으로 train_input~ test_target 을 생성한다.

# 층화샘플링
```

#표준화 전처리 
- 여기에서도 훈련 세트의 통계 값으로 테스트 세트를 변환해야한다는 점을 기억해라
- 데이터 가공
  + 숫자 결측치가 존재, 평균값으로 대체
  + 원본 데이터 평균 대치(x)
  + 훈련 데이터와 테스트 데이터 분리


```python
import numpy as np
np.mean(train_input[:, 2])
```




    31.269747899159658




```python
from sklearn.preprocessing import StandardScaler 
ss = StandardScaler()
ss.fit(train_input)
# ss.fit(test_input) (X)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

##모형 만들기
- K 최근접 이웃 


```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier(n_neighbors =3) # 5개가 아니라 근접 3개만
kn.fit(train_scaled, train_target) #훈련 시켜라
print(kn.score(train_scaled, train_target)) # 훈련 시키니 정확도가 어느정도니?
print(kn.score(test_scaled, test_target)) # 훈련 시킨 것을 바탕으로 실제로 test를 돌려보니 정확도가 어느정도니?
```

    0.8907563025210085
    0.85
    

- 타깃값 확인 


```python
print(kn.classes_) # 물고기 종류에는 어떤 것들이 있니?
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    


```python
test_scaled[:5] # 각 열은 특징들에 대한 점수라고 생각 총 행은 5개
```




    array([[-0.88741352, -0.91804565, -1.03098914, -0.90464451, -0.80762518],
           [-1.06924656, -1.50842035, -1.54345461, -1.58849582, -1.93803151],
           [-0.54401367,  0.35641402,  0.30663259, -0.8135697 , -0.65388895],
           [-0.34698097, -0.23396068, -0.22320459, -0.11905019, -0.12233464],
           [-0.68475132, -0.51509149, -0.58801052, -0.8998784 , -0.50124996]])




```python
print(kn.predict(test_scaled[:5])) # 5개에 대한 예측값을 말해봐라
```

    ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
    

- 5개 샘플에 대한 예측은 어떤 확률이냐?



```python
import numpy as np
proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba,decimals=4))
```

    [[0.     0.     1.     0.     0.     0.     0.    ]
     [0.     0.     0.     0.     0.     1.     0.    ]
     [0.     0.     0.     1.     0.     0.     0.    ]
     [0.     0.     0.6667 0.     0.3333 0.     0.    ]
     [0.     0.     0.6667 0.     0.3333 0.     0.    ]]
    

- 첫번째 클래스 Perch

[0.     0.    1.  0.   0.   0.   0.   }
- 네번 째 클래스 perch
  + 66.7 확률로 perch로 예측 



```python

```

##회귀식
-양변에 로그를 취함

- y=ax                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           

## 로지스틱 회귀로 이진 분류 수행 


```python
char_arr = np.array(['A','B','C','D','E']) # a,b,c,d,e에서
print(char_arr[[True, False, True, False, False]]) # 각 값에 참 거짓을 넣어주고, 참만 출력한다.
```

    ['A' 'C']
    


```python
bream_smelt_indexes = (train_target == 'Bream')|(train_target == 'Smelt') # 참 값만 고른다
train_bream_smelt = train_scaled[bream_smelt_indexes] # train_bream_smelt 에 bream이거나 smelt 인 값만 넣고~
target_bream_smelt = train_target[bream_smelt_indexes]# target 또한 같은 일을 반복한다.

train_scaled.shape, train_bream_smelt.shape # 모양을 확인하자 , train은 119개 test 는 33개가 있다.
```




    ((119, 5), (33, 5))



- 모델 만들기 


```python
from sklearn.linear_model import LogisticRegression 
lr= LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)# 훈련시켜라

```




    LogisticRegression()




```python
# 클래스를 예측
print(lr.predict(train_bream_smelt[:5])) # 상위 5개의 예측값을 출력해봐라
```

    ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
    

-확률 값 구하기 


```python
print(lr.predict_proba(train_bream_smelt[:5])) # 그럼 각 확률을 출력해봐라 breamd의 확률, smelt 의 확률
```

    [[0.99759855 0.00240145]
     [0.02735183 0.97264817]
     [0.99486072 0.00513928]
     [0.98584202 0.01415798]
     [0.99767269 0.00232731]]
    

- 분류 기준 : threshold 임계값 설정
도미 vs 빙어
[0.51,0.49],
[0.90,0.10]


```python
print(lr.classes_)
```

    ['Bream' 'Smelt']
    

- 계수와 절편 


```python
print(lr.coef_, lr.intercept_) # 앞의 5개는 weight의 계수, length의 계수 등등이고 마지막은 교차점이다.
```

    [[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
    


```python
decisions= lr.decision_function(train_bream_smelt[:5])
print(decisions)
```

    [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
    


```python
from scipy.special import expit # 시그모이드 함수를 적용한 이후의 모습이다. z5개를 돌렸기에 0~1사이의 값 5개가 나온 것이다.
print(expit(decisions))
```

    [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
    

## 다중 분류 수행하기


```python
#하이퍼 파라밑 세팅 => 모형을 튜닝
#모형 결과의 과대적합 또는 과소적합을 방지 하기 위한 것
lr = LogisticRegression(C=20,max_iter =1000)
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

    0.9327731092436975
    0.925
    

- 5개 샘플에 대한 예측 


```python
print(lr.predict(test_scaled[:5])) # 5개 샘플에대한 예측값
```

    ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']
    


```python
proba= lr.predict_proba(test_scaled[:5])
print(np.round(proba,decimals = 3))
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]
    

- 다중 분류일 경우 선형 방정식은 어떤 모습일까?
- 분류 7개, 컬럼 값 5개


```python
print(lr.coef_,lr.intercept_) #각각의 계수와 교차점
# z = 이 형식을 의미하는 것이다.

```

    [[-1.49002087 -1.02912886  2.59345551  7.70357682 -1.2007011 ]
     [ 0.19618235 -2.01068181 -3.77976834  6.50491489 -1.99482722]
     [ 3.56279745  6.34357182 -8.48971143 -5.75757348  3.79307308]
     [-0.10458098  3.60319431  3.93067812 -3.61736674 -1.75069691]
     [-1.40061442 -6.07503434  5.25969314 -0.87220069  1.86043659]
     [-1.38526214  1.49214574  1.39226167 -5.67734118 -4.40097523]
     [ 0.62149861 -2.32406685 -0.90660867  1.71599038  3.6936908 ]] [-0.09205179 -0.26290885  3.25101327 -0.14742956  2.65498283 -6.78782948
      1.38422358]
    


```python
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision,decimals=2))

# 하나의 검사값에 대해 7개의 z값이 나오고, 총 5개(검사한 갯수)가 나온다
```

    [[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
     [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
     [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
     [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
     [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]
    

- 2개 일 때는 시그모이드 함수를 사용해서 z 값을 0~1 사이의 값으로 변환하였지만 지금은 z값이 7개나 되므로 시그모이드가 아닌 소프트맥스 함수를 사용한다.


```python
from scipy.special import softmax # 시그모이드가 아닌 softmax를 사용한다.
proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]
    

## 평가지표
- 회귀 평가지표
- 결정계수
  + 1-(타깃-예측)^2의 합 / (타깃-평균)^2합
- MAE, MSE, RMSE
  + (실제-예측) = 오차
  + MAE(Mean Absolute Error): 오차의 절댓값의 평균
  + MSE(Mean Squared Error): 오차 제곱의 평균
  + RMSE(Root Mean Squared Error): MSE에 제곱근을 취한 값

- 좋은 모델이란?
  + 결정 계수: 1에 수렴하면 좋은 모델
  + MAE, MSE, RMSE : 0에 수렴하면 좋은 모델
  


```python
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
true = np.array([1, 2, 3, 2, 3, 5, 4, 6, 5, 6, 7, 8, 8]) # 실제값
preds = np.array([1, 1, 2, 2, 3, 4, 4, 5, 5, 7, 7, 6, 8]) # 예측값
#절대값 오차의 평균
mae= mean_absolute_error(true,preds)
print(mae)
# 제곱 오차의 평균
mse = mean_squared_error(true,preds)
print(mse)
# mse 제곱근
rmse = np.sqrt(mse)
print(rmse)
# 결정계수
r2= r2_score(true,preds)
print(r2)
```

    0.5384615384615384
    0.6923076923076923
    0.8320502943378437
    0.8617021276595744
    

## 분류 평가지표
- 오차행렬
- 실제 값
  +[빙어, 도미, 도미, 빙어, 도미]
  +[빙어, 빙어, 도미, 빙어, 빙어]
- TP(빙어를 빙어로 예측): 2
- TN(도미를 도미로 예측): 1
- FN(실제 도미, 예측 빙어): 2
- FP(실제 빙어, 예측 도미): 0
- 모형의 정확도 3/5 = 60 %

- TP, TN, FP, FN
  + 정확도: 전체에서 맞춘 갯수
  + 정밀도: 양성이라고 예측한 값 중 실제 양성인 값
  + 재현율: 실제 양성 값 중 양성으로 예측한 값의 비율
  + 로그손실
  + ROC Curve(=AUC)

