## 확률적 경사 하강법
- 점진적 학습(step, 보폭
- 학습률
- XGBoost, LightGBM, 딥러닝(이미지 분류, 자연어 처리,옵티마이저)

###신경망 이미지 데이터, 자연어
- 자율주행
- 자율주행 하루 데이터 1TB ->학습
- 한꺼번에 다 모델을 학습 어려움
  + 샘플링, 배치, 에포크, 오차(=손실 = loss)가 가장 작은 지점을 찾아야 한다.


```python

```

- 확률적으로 ,확률적 검사 하강법

  

## 손실함수
- 로지스틱 손실 함


```python
import pandas as pd
fish = pd.read_csv("https://bit.ly/fish_csv_data")
fish.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 159 entries, 0 to 158
    Data columns (total 6 columns):
     #   Column    Non-Null Count  Dtype  
    ---  ------    --------------  -----  
     0   Species   159 non-null    object 
     1   Weight    159 non-null    float64
     2   Length    159 non-null    float64
     3   Diagonal  159 non-null    float64
     4   Height    159 non-null    float64
     5   Width     159 non-null    float64
    dtypes: float64(5), object(1)
    memory usage: 7.6+ KB
    


```python

```

- 입력 데이터와 타깃 데이터 분리



```python
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
fish_input.shape, fish_target.shape
```




    ((159, 5), (159,))




```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42
)
```

- 훈련 세트와 테스트 세트의 특성 표준화
  + 무게, 길이, 대각선 길이, 높이, 너비
- 표준화 처리 진


```python
from sklearn.preprocessing import StandardScaler
ss= StandardScaler()
ss.fit(train_input)

train_scaled=ss.transform(train_input)
test_scaled = ss.transform(test_input)

# train_scaled[:5]
```

## 모델링
- 확률적 경사 하강법


```python
from sklearn.linear_model import SGDClassifier
sc = SGDClassifier(loss = 'log', max_iter = 10, random_state=42)

sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled,test_target))
```

    0.773109243697479
    0.775
    

    /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      ConvergenceWarning,
    

- partial_fit 메서드 사용하면 추가 학


```python
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled,train_target))
print(sc.score(test_scaled,test_target))
```

    0.8151260504201681
    0.85
    

## 에포크와 과대/과소 적합
- 에포크 숫자가 적으면 --> 덜 학습
- early_stopping 
  + 에포크 숫자를 1000, 손실 10, 9 ,8,....,3
  + 3에 도달한 시점이 150



```python
import numpy as np
sc = SGDClassifier(loss='log', random_state =42)
train_score = []
test_score=[]
classes = np.unique(train_target)
```

# 300번 에포크 훈련을 반복
# 훈련할 때마다, train_score, test_score를 추가한다.


```python
for _ in range(0,300):
  sc.partial_fit(train_scaled,train_target,classes=classes)
  train_score.append(sc.score(train_scaled, train_target))
  test_score.append(sc.score(test_scaled,test_target))
```


```python
# 시각화
import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
# 앞부분에 테스트 성능이 더 좋은것은 과소적합
# 100 이후부터는 과대적합?
```


    
![png](/images/day0704/output_20_0.png)
    


## XGBoost, LightGBM 코드
- train -loss, train-accuracy, test-loss, test-accuracy


```python

```


```python
import pandas as pd
wine=pd.read_csv('https://bit.ly/wine_csv_data')

```


```python
wine.head()
```





  <div id="df-3bf02ac1-ddc8-4efa-a92e-c38f41acaaf5">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>alcohol</th>
      <th>sugar</th>
      <th>pH</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.4</td>
      <td>1.9</td>
      <td>3.51</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9.8</td>
      <td>2.6</td>
      <td>3.20</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9.8</td>
      <td>2.3</td>
      <td>3.26</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9.8</td>
      <td>1.9</td>
      <td>3.16</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>9.4</td>
      <td>1.9</td>
      <td>3.51</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-3bf02ac1-ddc8-4efa-a92e-c38f41acaaf5')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-3bf02ac1-ddc8-4efa-a92e-c38f41acaaf5 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-3bf02ac1-ddc8-4efa-a92e-c38f41acaaf5');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
wine.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 6497 entries, 0 to 6496
    Data columns (total 4 columns):
     #   Column   Non-Null Count  Dtype  
    ---  ------   --------------  -----  
     0   alcohol  6497 non-null   float64
     1   sugar    6497 non-null   float64
     2   pH       6497 non-null   float64
     3   class    6497 non-null   float64
    dtypes: float64(4)
    memory usage: 203.2 KB
    


```python
wine.describe()
```





  <div id="df-bfadddc7-f809-497f-8df3-d3dd992f496e">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>alcohol</th>
      <th>sugar</th>
      <th>pH</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>6497.000000</td>
      <td>6497.000000</td>
      <td>6497.000000</td>
      <td>6497.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>10.491801</td>
      <td>5.443235</td>
      <td>3.218501</td>
      <td>0.753886</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.192712</td>
      <td>4.757804</td>
      <td>0.160787</td>
      <td>0.430779</td>
    </tr>
    <tr>
      <th>min</th>
      <td>8.000000</td>
      <td>0.600000</td>
      <td>2.720000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>9.500000</td>
      <td>1.800000</td>
      <td>3.110000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>10.300000</td>
      <td>3.000000</td>
      <td>3.210000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>11.300000</td>
      <td>8.100000</td>
      <td>3.320000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>14.900000</td>
      <td>65.800000</td>
      <td>4.010000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-bfadddc7-f809-497f-8df3-d3dd992f496e')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-bfadddc7-f809-497f-8df3-d3dd992f496e button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-bfadddc7-f809-497f-8df3-d3dd992f496e');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
data = wine[['alcohol','sugar','pH']].to_numpy()
target = wine['class'].to_numpy()


```


```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data,target,test_size=0.2,random_state=42)
```


```python
print(train_input.shape,test_input.shape)
```

    (5197, 3) (1300, 3)
    

- StandardScalar 클래스를 사용해 훈련 세트 전처


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled,train_target))
print(lr.score(test_scaled, test_target))
```

    0.7808350971714451
    0.7776923076923077
    


```python
print(lr.coef_,lr.intercept_)
```

    [[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]
    


```python
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
dt = DecisionTreeClassifier(max_depth=7,random_state =42)
dt.fit(train_scaled,train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled,test_target))
plt.figure(figsize=(10,7))
plot_tree(dt,max_depth=10,filled=True, feature_names=['alcohol','sugar','pH'])
plt.show()
```

    0.8895516644217818
    0.8630769230769231
    


    
![png](/images/day0704/output_34_1.png)
    



```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth =1, filled=True, feature_names=['alcohol','sugar','pH'])
plt.show()
```


    
![png](/images/day0704/output_35_0.png)
    


- 불순도
  + 비율
  + 레드와인 5:5 화이트와인
  + 한 범주 안에서 서로 다른 데이터가 얼마나 섞여 있는지 나타냄
    + 흰새과 검은색이 각각 4-개씩 있다.
    + 불순도 최대 0.5
    + 흰색과 검은색이 완전 100% 분리가 됨
    + 흰색 노드 불순도 최소 0
    + 검은색 노드 불순도 최소0
  - 엔드로피
    + 불확실한 정도를 의미함, 0~1 사이
    + 흰색과 검은색이 각각 50개씩 섞여 있다.
      + 엔트로피 최대 1
    + 흰색과 검은색이 완전 100% 분리됨




```python
print(dt.feature_importances_)
```

    [0.16949576 0.67274329 0.15776095]
    

- 현업에서 DEcisionTreeClassifier
- 랜덤 포레스트, XGBoost 하이퍼파라미터 매우 많음

## 검증 세트
- 훈련세트와 테스트세트
- 훈련: 교과서 공부하는 것 훈련세트, 모의평가
- 검증: 강남대성 모의고사 문제지
- 테스트: 6월 9월
- 실전: 수능


```python
import pandas as pd
wine=pd.read_csv('https://bit.ly/wine_csv_data')

```


```python
data = wine[['alcohol','sugar','pH']].to_numpy()
target = wine['class'].to_numpy()
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data,target,test_size=0.2,random_state=42)
```


```python
sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size=0.2,random_state=42) 
```


```python
print(sub_input.shape, val_input.shape)
```

    (4157, 3) (1040, 3)
    


```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state =42)
dt.fit(sub_input, sub_target)
print("훈련성과:",dt.score(sub_input, sub_target))
print("검증성과:",dt.score(val_input, val_target))
print("마지막 최종:",dt.score(test_input, test_target))
```

    훈련성과: 0.9971133028626413
    검증성과: 0.864423076923077
    마지막 최종: 0.8569230769230769
    

## 교차 검증
- 데이터 셋을 반복 분할
- For loop
- 샘플링 편향적일 수 있음
- 교차검증을 한다고 해서, 정확도가 무조건 올라가는 것은 아니다.
- 모형을 안정적으로 만들어준다.
  + 과대적합 방지


```python
import numpy as np
from sklearn.model_selection import KFold

df = np.array([1,2,3,4,5,6,7,8,9,10])

#데이터를 K폴드로 나눈다.
folds = KFold(n_splits = 5,shuffle =True)
for train_idx, valid_idx in folds.split(df):
  print(f'훈련 데이터: {df[train_idx]},검증데이터 : {df[valid_idx]}')
  
```

    훈련 데이터: [ 1  2  3  4  5  6  8 10],검증데이터 : [7 9]
    훈련 데이터: [ 1  2  3  4  6  7  9 10],검증데이터 : [5 8]
    훈련 데이터: [ 3  4  5  6  7  8  9 10],검증데이터 : [1 2]
    훈련 데이터: [1 2 3 5 6 7 8 9],검증데이터 : [ 4 10]
    훈련 데이터: [ 1  2  4  5  7  8  9 10],검증데이터 : [3 6]
    


```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
print("평균: ", np.mean(scores['test_score']))
```

    {'fit_time': array([0.01039791, 0.01018906, 0.01092386, 0.01160955, 0.01004958]), 'score_time': array([0.0053134 , 0.00104594, 0.00103641, 0.00105906, 0.00095129]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
    평균:  0.855300214703487
    

- StratifiedKFold 사용 


```python
from sklearn.model_selection import StratifiedKFold
scores = cross_validate(dt, train_input, train_target,cv=StratifiedKFold())
print(scores)
print("평균 : ",np.mean(scores['test_score']))
```

    {'fit_time': array([0.00961757, 0.0068903 , 0.00719047, 0.00706291, 0.00724101]), 'score_time': array([0.00066686, 0.00058341, 0.0006268 , 0.00061035, 0.00061965]), 'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
    평균 :  0.855300214703487
    


```python
splitter = StratifiedKFold(n_splits = 10, shuffle = True, random_state =42)
scores = cross_validate(dt, train_input, train_target, cv= splitter)
print(np.mean(scores['test_score']))
```

    0.8574181117533719
    

## 하이퍼파라미터 튜닝(모델이 학습할 수 없어서 사용자가 지정해야만 하는 파라미터)
- 그리드 서치
  + 사람이 수동적으로 입력
  + max_depth:[1,3,5]
- 랜덤 서치
  + 사람이 범위만 지정
  + max_depth: 1부터 10 사이 아무거나
- 베이지안 옵티마이제이션
- 사람의 개입 없이 하이퍼파라미터 튜닝을 자동으로 수행하는 기술을 AutoML이라고 함
  + 예: PyCaret
- 각 모델마다 적게는 1~2개에서, 많게는 5~6개의 매개변수를 제공한다.
  + XGBoost 100개
- 하이퍼파라미터와 동시에 교차검증을 수행

- 교차검증 5번
- 교차검증 1번 동 때, Max Depth 3번 적용
- 총 결과값 3 X 5 X 2 나옴
- Max Depth = 1,3,7
- Criterion = gini,entropy


```python
from sklearn.model_selection import GridSearchCV
params={
    
    'min_impurity_decrease': [0.0001, 0.0002,0.0003,0.0004,0.0005]}
gs = GridSearchCV(DecisionTreeClassifier(random_state =42), params, n_jobs=-1)
gs.fit(train_input, train_target)


```




    GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,
                 param_grid={'min_impurity_decrease': [0.0001, 0.0002, 0.0003,
                                                       0.0004, 0.0005]})




```python
print("best : ", gs.best_estimator_)
dt = gs.best_estimator_
print(dt.score(train_input, train_target))
```

    best :  DecisionTreeClassifier(min_impurity_decrease=0.0001, random_state=42)
    0.9615162593804117
    


```python
print(gs.best_params_)
```

    {'min_impurity_decrease': 0.0001}
    


```python
print(gs.cv_results_['mean_test_score'])
```

    [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605]
    


```python
best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])
```

    {'min_impurity_decrease': 0.0001}
    


```python
params = {'min_impurity_decrease': np.arange(0.0001,0.001,0.0001),
          'max_depth': range(5,20,1),
          'min_samples_split':range(2,100,10)
          }
```


```python
gs = GridSearchCV(DecisionTreeClassifier(random_state = 42), params, n_jobs = -1)
gs.fit(train_input, train_target)
```




    GridSearchCV(estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,
                 param_grid={'max_depth': range(5, 20),
                             'min_impurity_decrease': array([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008,
           0.0009]),
                             'min_samples_split': range(2, 100, 10)})




```python
print(gs.best_params_)
```

    {'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}
    


```python
print(np.max(gs.cv_results_['mean_test_score']))
```

    0.8683865773302731
    


```python
from scipy.stats import uniform, randint
```


```python
rgen = randint(0,10)
rgen.rvs(10)
```




    array([1, 6, 4, 3, 9, 6, 2, 8, 3, 0])




```python
np.unique(rgen.rvs(1000), return_counts = True)
```




    (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
     array([ 92, 107,  99,  95, 103, 120, 104,  86,  85, 109]))




```python
ugen = uniform(0,1)
ugen.rvs(10)
```




    array([0.74572568, 0.67723448, 0.2947353 , 0.65314677, 0.71850518,
           0.45742993, 0.13068961, 0.15581053, 0.29705039, 0.32323251])




```python
params = {'min_impurity_decrease': uniform(0.0001,0.001),
          'max_depth': randint(20,50),
          'min_samples_split': randint(2,25),
          'min_samples_leaf': randint(1,25),}
```


```python
from sklearn.model_selection import RandomizedSearchCV
gs = RandomizedSearchCV(DecisionTreeClassifier(random_state =42), params,
                        n_iter=100, n_jobs =-1, random_state=42)
gs.fit(train_input, train_target)
```




    RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=42),
                       n_iter=100, n_jobs=-1,
                       param_distributions={'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe6693b8250>,
                                            'min_impurity_decrease': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe66a065110>,
                                            'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe6693b8bd0>,
                                            'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe6693b83d0>},
                       random_state=42)




```python
print(gs.best_params_)
```

    {'max_depth': 39, 'min_impurity_decrease': 0.00034102546602601173, 'min_samples_leaf': 7, 'min_samples_split': 13}
    


```python
print(np.max(gs.cv_results_['mean_test_score']))
```

    0.8695428296438884
    


```python
dt = gs.best_estimator_
print(dt.score(test_input, test_target))
```

    0.86
    
